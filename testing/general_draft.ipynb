{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.0 Installing neccessary libraries",
   "id": "9421b67f47af3a4b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-20T14:13:21.981209Z",
     "start_time": "2024-07-20T14:13:19.952147Z"
    }
   },
   "source": "%pip install -r requirements.txt",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==24.1.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 2)) (24.1.2)\n",
      "Requirement already satisfied: jupyter==1.0.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.0)\n",
      "Requirement already satisfied: scipy==1.14.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 8)) (1.14.0)\n",
      "Requirement already satisfied: numpy==2.0.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 11)) (2.0.0)\n",
      "Requirement already satisfied: pandas==2.2.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 14)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib==3.9.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 17)) (3.9.1)\n",
      "Requirement already satisfied: scikit-learn==1.5.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from -r requirements.txt (line 20)) (1.5.1)\n",
      "Requirement already satisfied: notebook in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (7.3.0a1)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (5.5.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (7.16.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter==1.0.0->-r requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 14)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 14)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from pandas==2.2.2->-r requirements.txt (line 14)) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from matplotlib==3.9.1->-r requirements.txt (line 17)) (3.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 20)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 20)) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (1.8.2)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (8.26.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (6.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 5)) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 5)) (3.0.11)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 5)) (3.0.47)\n",
      "Requirement already satisfied: pygments in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 5)) (2.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (4.13.0b2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (0.8.0rc2)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (3.1.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.14.2)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.4,>=4.3.0a1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (4.3.0a2)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.2.4)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from bleach!=5.0.0->nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (4.2.2)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (306)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (4.4.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.20.0)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.0.13)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.0.0b0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.2.5)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (71.0.4)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.15.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.9.25)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (2.20.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 5)) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 5)) (2.5)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (21.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from rfc3986[idna2008]<2,>=1.3->httpx>=0.25.0->jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: httpcore<0.14.0,>=0.13.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.13.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (6.0.2rc1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter==1.0.0->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from httpcore<0.14.0,>=0.13.3->httpx>=0.25.0->jupyterlab<4.4,>=4.3.0a1->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (0.12.0)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5))\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: fqdn in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (24.6.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.17.0rc1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\hp\\downloads\\machine learning assignment\\ml-venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook->jupyter==1.0.0->-r requirements.txt (line 5)) (2.9.0.20240316)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Installing collected packages: anyio\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.4.0\n",
      "    Uninstalling anyio-4.4.0:\n",
      "      Successfully uninstalled anyio-4.4.0\n",
      "Successfully installed anyio-3.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T14:14:05.847206Z",
     "start_time": "2024-07-20T14:14:05.201452Z"
    }
   },
   "cell_type": "code",
   "source": "%pip list",
   "id": "109813ee05e564f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     3.7.1\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.15.0\n",
      "beautifulsoup4            4.13.0b2\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.7.4\n",
      "cffi                      1.17.0rc1\n",
      "charset-normalizer        3.3.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.2.1\n",
      "cycler                    0.12.1\n",
      "debugpy                   1.8.2\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.8.0rc2\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.20.0\n",
      "fonttools                 4.53.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.12.0\n",
      "httpcore                  0.13.7\n",
      "httpx                     1.0.0b0\n",
      "idna                      3.7\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.26.0\n",
      "ipywidgets                8.1.3\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "joblib                    1.4.2\n",
      "json5                     0.9.25\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.2\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.0a2\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.11\n",
      "kiwisolver                1.4.5\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib                3.9.1\n",
      "matplotlib-inline         0.1.7\n",
      "mistune                   3.0.2\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook                  7.3.0a1\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.0.0\n",
      "overrides                 7.7.0\n",
      "packaging                 24.1\n",
      "pandas                    2.2.2\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pillow                    10.4.0\n",
      "pip                       24.1.2\n",
      "platformdirs              4.2.2\n",
      "prometheus_client         0.20.0\n",
      "prompt_toolkit            3.0.47\n",
      "psutil                    6.0.0\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.22\n",
      "Pygments                  2.18.0\n",
      "pyparsing                 3.1.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.1\n",
      "pywin32                   306\n",
      "pywinpty                  2.0.13\n",
      "PyYAML                    6.0.2rc1\n",
      "pyzmq                     26.0.3\n",
      "qtconsole                 5.5.2\n",
      "QtPy                      2.4.1\n",
      "referencing               0.35.1\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986                   1.5.0\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.19.0\n",
      "scikit-learn              1.5.1\n",
      "scipy                     1.14.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                71.0.4\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "threadpoolctl             3.5.0\n",
      "tinycss2                  1.3.0\n",
      "tornado                   6.4.1\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.12.2\n",
      "tzdata                    2024.1\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.2\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.6.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T15:03:44.564023Z",
     "start_time": "2024-09-13T15:03:43.774647Z"
    }
   },
   "cell_type": "code",
   "source": "pip freeze > requirements.txt",
   "id": "850ec22db1e0258d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fa28148205848f5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T17:55:50.683413Z",
     "start_time": "2024-09-08T17:55:33.097707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, MeanShift, Birch, HDBSCAN, OPTICS\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "df_umap = pd.read_csv(r'./dimension_reduced_data/knn3_umap3.csv', index_col = 'CUST_ID')\n",
    "\n",
    "df_clusters = pd.DataFrame(index = df_umap.index)\n",
    "df_scores = pd.DataFrame(index = ['silhouette', 'davies_bouldin', 'calinski_harabasz'])\n",
    "\n",
    "\n",
    "for n_clusters in range(2, 11):\n",
    "    for linkage in ['ward', 'complete', 'average', 'single']:\n",
    "        agglomerative_model = AgglomerativeClustering(n_clusters = n_clusters, metric = 'euclidean', linkage = linkage).fit(df_umap)\n",
    "        \n",
    "        df_clusters[f'agg_ncluster{n_clusters}_linkage{linkage}'] = agglomerative_model.labels_\n",
    "        \n",
    "        df_scores[f'agg_ncluster{n_clusters}_linkage{linkage}'] = [silhouette_score(df_umap, agglomerative_model.labels_), davies_bouldin_score(df_umap, agglomerative_model.labels_), calinski_harabasz_score(df_umap, agglomerative_model.labels_)]"
   ],
   "id": "2e34388357039226",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:01:11.183584Z",
     "start_time": "2024-09-08T17:58:51.157383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import estimate_bandwidth\n",
    "import numpy as np\n",
    "\n",
    "for quantile in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    # Bandwidth setting\n",
    "    bandwidth = estimate_bandwidth(df_umap, quantile=quantile)\n",
    "\n",
    "    # Creating and fitting the Mean Shift model\n",
    "    ms_model= MeanShift(bandwidth=bandwidth).fit(df_umap)\n",
    "\n",
    "    df_clusters[f'ms_quantile{quantile}'] = ms_model.labels_\n",
    "\n",
    "    if len(np.unique(ms_model.labels_)) > 1:\n",
    "        df_scores[f'ms_quantile{quantile}'] = [silhouette_score(df_umap, ms_model.labels_), davies_bouldin_score(df_umap, ms_model.labels_), calinski_harabasz_score(df_umap, ms_model.labels_)]\n",
    "    else:\n",
    "        df_scores[f'ms_quantile{quantile}'] = [-1, -1, -1]"
   ],
   "id": "a4a215e21c3d1181",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:04:24.907532Z",
     "start_time": "2024-09-08T18:03:24.949309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for threshold in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    for factor in [10, 20, 30, 40, 50]:\n",
    "        for ncluster in [2, 3, 4, 5, 6]:\n",
    "            # Creating and fitting the Birch model\n",
    "            birch_model = Birch(threshold = threshold, branching_factor = factor, n_clusters = ncluster).fit(df_umap)\n",
    "\n",
    "            df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
    "\n",
    "            df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]"
   ],
   "id": "fa5fe2a283677ee9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = birch_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\929092676.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'birch_threshold{threshold}_factor{factor}_ncluster{ncluster}'] = [silhouette_score(df_umap, birch_model.labels_), davies_bouldin_score(df_umap, birch_model.labels_), calinski_harabasz_score(df_umap, birch_model.labels_)]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:08:36.285516Z",
     "start_time": "2024-09-08T18:08:13.436156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for min_cluster_size in range(5, 51, 5):\n",
    "    for min_sample in range(5, 11):\n",
    "        # Creating and fitting the HDBSCAN model\n",
    "        hdbscan_model = HDBSCAN(min_cluster_size = min_cluster_size, min_samples = min_sample).fit(df_umap)\n",
    "\n",
    "        df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
    "\n",
    "        df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]"
   ],
   "id": "e8343643933a0462",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = hdbscan_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\2643497086.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'hdbscan_mincluster{min_cluster_size}_minsample{min_sample}'] = [silhouette_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), davies_bouldin_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1]), calinski_harabasz_score(df_umap[hdbscan_model.labels_ != -1], hdbscan_model.labels_[hdbscan_model.labels_ != -1])]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:11:32.304483Z",
     "start_time": "2024-09-08T18:10:39.275089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for min_samp in [30, 60, 80, 120, 140]:\n",
    "    for xi in [0.0005, 0.005, 0.01, 0.02, 0.05]:\n",
    "        # Creating and fitting the OPTICS model\n",
    "        optics_model = OPTICS(min_samples = min_samp, xi = xi).fit(df_umap)\n",
    "\n",
    "        df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
    "        \n",
    "        df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]"
   ],
   "id": "9bb17677be2b0fb2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_clusters[f'optics_minsamp{min_samp}_xi{xi}'] = optics_model.labels_\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13732\\15050925.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_scores[f'optics_minsamp{min_samp}_xi{xi}'] = [silhouette_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), davies_bouldin_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1]), calinski_harabasz_score(df_umap[optics_model.labels_ != -1], optics_model.labels_[optics_model.labels_ != -1])]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:11:38.628136Z",
     "start_time": "2024-09-08T18:11:38.583859Z"
    }
   },
   "cell_type": "code",
   "source": "df_clusters",
   "id": "8514318a17a4e0e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         agg_ncluster2_linkageward  agg_ncluster2_linkagecomplete  \\\n",
       "CUST_ID                                                             \n",
       "C17875                           0                              0   \n",
       "C16296                           0                              1   \n",
       "C17219                           0                              1   \n",
       "C13108                           0                              0   \n",
       "C13576                           0                              0   \n",
       "...                            ...                            ...   \n",
       "C14210                           1                              0   \n",
       "C17858                           0                              0   \n",
       "C12903                           0                              0   \n",
       "C10444                           1                              0   \n",
       "C10613                           1                              0   \n",
       "\n",
       "         agg_ncluster2_linkageaverage  agg_ncluster2_linkagesingle  \\\n",
       "CUST_ID                                                              \n",
       "C17875                              0                            0   \n",
       "C16296                              0                            0   \n",
       "C17219                              0                            0   \n",
       "C13108                              0                            0   \n",
       "C13576                              0                            0   \n",
       "...                               ...                          ...   \n",
       "C14210                              0                            0   \n",
       "C17858                              0                            0   \n",
       "C12903                              0                            0   \n",
       "C10444                              0                            0   \n",
       "C10613                              0                            0   \n",
       "\n",
       "         agg_ncluster3_linkageward  agg_ncluster3_linkagecomplete  \\\n",
       "CUST_ID                                                             \n",
       "C17875                           2                              2   \n",
       "C16296                           1                              1   \n",
       "C17219                           1                              1   \n",
       "C13108                           1                              2   \n",
       "C13576                           1                              2   \n",
       "...                            ...                            ...   \n",
       "C14210                           0                              0   \n",
       "C17858                           2                              2   \n",
       "C12903                           2                              2   \n",
       "C10444                           0                              0   \n",
       "C10613                           0                              0   \n",
       "\n",
       "         agg_ncluster3_linkageaverage  agg_ncluster3_linkagesingle  \\\n",
       "CUST_ID                                                              \n",
       "C17875                              0                            0   \n",
       "C16296                              0                            0   \n",
       "C17219                              0                            0   \n",
       "C13108                              0                            0   \n",
       "C13576                              0                            0   \n",
       "...                               ...                          ...   \n",
       "C14210                              2                            0   \n",
       "C17858                              0                            0   \n",
       "C12903                              0                            0   \n",
       "C10444                              2                            0   \n",
       "C10613                              2                            0   \n",
       "\n",
       "         agg_ncluster4_linkageward  agg_ncluster4_linkagecomplete  ...  \\\n",
       "CUST_ID                                                            ...   \n",
       "C17875                           2                              0  ...   \n",
       "C16296                           0                              1  ...   \n",
       "C17219                           0                              1  ...   \n",
       "C13108                           0                              0  ...   \n",
       "C13576                           0                              0  ...   \n",
       "...                            ...                            ...  ...   \n",
       "C14210                           1                              2  ...   \n",
       "C17858                           2                              0  ...   \n",
       "C12903                           2                              0  ...   \n",
       "C10444                           1                              2  ...   \n",
       "C10613                           1                              2  ...   \n",
       "\n",
       "         optics_minsamp120_xi0.0005  optics_minsamp120_xi0.005  \\\n",
       "CUST_ID                                                          \n",
       "C17875                            0                          0   \n",
       "C16296                           -1                         -1   \n",
       "C17219                            4                          4   \n",
       "C13108                           -1                         -1   \n",
       "C13576                           -1                         -1   \n",
       "...                             ...                        ...   \n",
       "C14210                            5                          5   \n",
       "C17858                            0                          0   \n",
       "C12903                            0                          0   \n",
       "C10444                            6                          6   \n",
       "C10613                            6                          6   \n",
       "\n",
       "         optics_minsamp120_xi0.01  optics_minsamp120_xi0.02  \\\n",
       "CUST_ID                                                       \n",
       "C17875                          0                         0   \n",
       "C16296                         -1                        -1   \n",
       "C17219                          4                         1   \n",
       "C13108                         -1                        -1   \n",
       "C13576                         -1                        -1   \n",
       "...                           ...                       ...   \n",
       "C14210                          5                         2   \n",
       "C17858                          0                         0   \n",
       "C12903                          0                         0   \n",
       "C10444                         -1                         2   \n",
       "C10613                         -1                         2   \n",
       "\n",
       "         optics_minsamp120_xi0.05  optics_minsamp140_xi0.0005  \\\n",
       "CUST_ID                                                         \n",
       "C17875                          0                           0   \n",
       "C16296                         -1                          -1   \n",
       "C17219                         -1                          -1   \n",
       "C13108                         -1                          -1   \n",
       "C13576                         -1                          -1   \n",
       "...                           ...                         ...   \n",
       "C14210                          1                           4   \n",
       "C17858                          0                           0   \n",
       "C12903                          0                           0   \n",
       "C10444                          1                           5   \n",
       "C10613                          1                           5   \n",
       "\n",
       "         optics_minsamp140_xi0.005  optics_minsamp140_xi0.01  \\\n",
       "CUST_ID                                                        \n",
       "C17875                           0                         0   \n",
       "C16296                          -1                        -1   \n",
       "C17219                          -1                        -1   \n",
       "C13108                          -1                        -1   \n",
       "C13576                          -1                        -1   \n",
       "...                            ...                       ...   \n",
       "C14210                          -1                        -1   \n",
       "C17858                           0                         0   \n",
       "C12903                           0                         0   \n",
       "C10444                           5                        -1   \n",
       "C10613                           5                        -1   \n",
       "\n",
       "         optics_minsamp140_xi0.02  optics_minsamp140_xi0.05  \n",
       "CUST_ID                                                      \n",
       "C17875                          0                         0  \n",
       "C16296                         -1                        -1  \n",
       "C17219                         -1                        -1  \n",
       "C13108                         -1                        -1  \n",
       "C13576                         -1                        -1  \n",
       "...                           ...                       ...  \n",
       "C14210                          1                        -1  \n",
       "C17858                          0                         0  \n",
       "C12903                          0                         0  \n",
       "C10444                          1                        -1  \n",
       "C10613                          1                        -1  \n",
       "\n",
       "[3500 rows x 251 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_ncluster2_linkageward</th>\n",
       "      <th>agg_ncluster2_linkagecomplete</th>\n",
       "      <th>agg_ncluster2_linkageaverage</th>\n",
       "      <th>agg_ncluster2_linkagesingle</th>\n",
       "      <th>agg_ncluster3_linkageward</th>\n",
       "      <th>agg_ncluster3_linkagecomplete</th>\n",
       "      <th>agg_ncluster3_linkageaverage</th>\n",
       "      <th>agg_ncluster3_linkagesingle</th>\n",
       "      <th>agg_ncluster4_linkageward</th>\n",
       "      <th>agg_ncluster4_linkagecomplete</th>\n",
       "      <th>...</th>\n",
       "      <th>optics_minsamp120_xi0.0005</th>\n",
       "      <th>optics_minsamp120_xi0.005</th>\n",
       "      <th>optics_minsamp120_xi0.01</th>\n",
       "      <th>optics_minsamp120_xi0.02</th>\n",
       "      <th>optics_minsamp120_xi0.05</th>\n",
       "      <th>optics_minsamp140_xi0.0005</th>\n",
       "      <th>optics_minsamp140_xi0.005</th>\n",
       "      <th>optics_minsamp140_xi0.01</th>\n",
       "      <th>optics_minsamp140_xi0.02</th>\n",
       "      <th>optics_minsamp140_xi0.05</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CUST_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C17875</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C16296</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C17219</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C13576</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C14210</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C17858</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C12903</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C10444</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C10613</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 251 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:11:50.528306Z",
     "start_time": "2024-09-08T18:11:50.482312Z"
    }
   },
   "cell_type": "code",
   "source": "df_scores",
   "id": "1fc67ad0b364a90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   agg_ncluster2_linkageward  agg_ncluster2_linkagecomplete  \\\n",
       "silhouette                          0.366102                       0.088799   \n",
       "davies_bouldin                      1.139835                       2.024310   \n",
       "calinski_harabasz                2088.770904                     430.282661   \n",
       "\n",
       "                   agg_ncluster2_linkageaverage  agg_ncluster2_linkagesingle  \\\n",
       "silhouette                             0.309151                     0.309151   \n",
       "davies_bouldin                         0.541395                     0.541395   \n",
       "calinski_harabasz                    106.937648                   106.937648   \n",
       "\n",
       "                   agg_ncluster3_linkageward  agg_ncluster3_linkagecomplete  \\\n",
       "silhouette                          0.387961                       0.269658   \n",
       "davies_bouldin                      0.929431                       1.377573   \n",
       "calinski_harabasz                2007.486411                    1640.478566   \n",
       "\n",
       "                   agg_ncluster3_linkageaverage  agg_ncluster3_linkagesingle  \\\n",
       "silhouette                             0.320291                     0.184405   \n",
       "davies_bouldin                         0.922123                     0.795726   \n",
       "calinski_harabasz                   1174.836952                   362.911648   \n",
       "\n",
       "                   agg_ncluster4_linkageward  agg_ncluster4_linkagecomplete  \\\n",
       "silhouette                          0.428147                       0.321796   \n",
       "davies_bouldin                      0.756479                       1.180261   \n",
       "calinski_harabasz                2142.713459                    1729.097230   \n",
       "\n",
       "                   ...  optics_minsamp120_xi0.0005  optics_minsamp120_xi0.005  \\\n",
       "silhouette         ...                    0.548577                   0.574236   \n",
       "davies_bouldin     ...                    0.658468                   0.614286   \n",
       "calinski_harabasz  ...                 3558.345344                3389.844066   \n",
       "\n",
       "                   optics_minsamp120_xi0.01  optics_minsamp120_xi0.02  \\\n",
       "silhouette                         0.593377                  0.604613   \n",
       "davies_bouldin                     0.593893                  0.515698   \n",
       "calinski_harabasz               3307.671563               3109.942236   \n",
       "\n",
       "                   optics_minsamp120_xi0.05  optics_minsamp140_xi0.0005  \\\n",
       "silhouette                         0.609850                    0.577771   \n",
       "davies_bouldin                     0.530535                    0.588762   \n",
       "calinski_harabasz               3381.030928                 3770.362430   \n",
       "\n",
       "                   optics_minsamp140_xi0.005  optics_minsamp140_xi0.01  \\\n",
       "silhouette                          0.580939                  0.637180   \n",
       "davies_bouldin                      0.586676                  0.491374   \n",
       "calinski_harabasz                3688.454503               3412.694977   \n",
       "\n",
       "                   optics_minsamp140_xi0.02  optics_minsamp140_xi0.05  \n",
       "silhouette                         0.608350                  0.692485  \n",
       "davies_bouldin                     0.531678                  0.445922  \n",
       "calinski_harabasz               3363.395816               3656.803450  \n",
       "\n",
       "[3 rows x 251 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg_ncluster2_linkageward</th>\n",
       "      <th>agg_ncluster2_linkagecomplete</th>\n",
       "      <th>agg_ncluster2_linkageaverage</th>\n",
       "      <th>agg_ncluster2_linkagesingle</th>\n",
       "      <th>agg_ncluster3_linkageward</th>\n",
       "      <th>agg_ncluster3_linkagecomplete</th>\n",
       "      <th>agg_ncluster3_linkageaverage</th>\n",
       "      <th>agg_ncluster3_linkagesingle</th>\n",
       "      <th>agg_ncluster4_linkageward</th>\n",
       "      <th>agg_ncluster4_linkagecomplete</th>\n",
       "      <th>...</th>\n",
       "      <th>optics_minsamp120_xi0.0005</th>\n",
       "      <th>optics_minsamp120_xi0.005</th>\n",
       "      <th>optics_minsamp120_xi0.01</th>\n",
       "      <th>optics_minsamp120_xi0.02</th>\n",
       "      <th>optics_minsamp120_xi0.05</th>\n",
       "      <th>optics_minsamp140_xi0.0005</th>\n",
       "      <th>optics_minsamp140_xi0.005</th>\n",
       "      <th>optics_minsamp140_xi0.01</th>\n",
       "      <th>optics_minsamp140_xi0.02</th>\n",
       "      <th>optics_minsamp140_xi0.05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>silhouette</th>\n",
       "      <td>0.366102</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.309151</td>\n",
       "      <td>0.309151</td>\n",
       "      <td>0.387961</td>\n",
       "      <td>0.269658</td>\n",
       "      <td>0.320291</td>\n",
       "      <td>0.184405</td>\n",
       "      <td>0.428147</td>\n",
       "      <td>0.321796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548577</td>\n",
       "      <td>0.574236</td>\n",
       "      <td>0.593377</td>\n",
       "      <td>0.604613</td>\n",
       "      <td>0.609850</td>\n",
       "      <td>0.577771</td>\n",
       "      <td>0.580939</td>\n",
       "      <td>0.637180</td>\n",
       "      <td>0.608350</td>\n",
       "      <td>0.692485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davies_bouldin</th>\n",
       "      <td>1.139835</td>\n",
       "      <td>2.024310</td>\n",
       "      <td>0.541395</td>\n",
       "      <td>0.541395</td>\n",
       "      <td>0.929431</td>\n",
       "      <td>1.377573</td>\n",
       "      <td>0.922123</td>\n",
       "      <td>0.795726</td>\n",
       "      <td>0.756479</td>\n",
       "      <td>1.180261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.658468</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>0.515698</td>\n",
       "      <td>0.530535</td>\n",
       "      <td>0.588762</td>\n",
       "      <td>0.586676</td>\n",
       "      <td>0.491374</td>\n",
       "      <td>0.531678</td>\n",
       "      <td>0.445922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calinski_harabasz</th>\n",
       "      <td>2088.770904</td>\n",
       "      <td>430.282661</td>\n",
       "      <td>106.937648</td>\n",
       "      <td>106.937648</td>\n",
       "      <td>2007.486411</td>\n",
       "      <td>1640.478566</td>\n",
       "      <td>1174.836952</td>\n",
       "      <td>362.911648</td>\n",
       "      <td>2142.713459</td>\n",
       "      <td>1729.097230</td>\n",
       "      <td>...</td>\n",
       "      <td>3558.345344</td>\n",
       "      <td>3389.844066</td>\n",
       "      <td>3307.671563</td>\n",
       "      <td>3109.942236</td>\n",
       "      <td>3381.030928</td>\n",
       "      <td>3770.362430</td>\n",
       "      <td>3688.454503</td>\n",
       "      <td>3412.694977</td>\n",
       "      <td>3363.395816</td>\n",
       "      <td>3656.803450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 251 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T18:12:36.752288Z",
     "start_time": "2024-09-08T18:12:36.495752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_clusters.to_csv(r'./cluster_data/clusters.csv')\n",
    "df_scores.to_csv(r'./cluster_data/scores.csv')"
   ],
   "id": "ae05f571f43f2e3c",
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
